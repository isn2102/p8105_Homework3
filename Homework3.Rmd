---
title: "Homework3"
author: "Isabel Nelson"
date: "10/6/2020"
output: github_document
---

```{r setup, include=FALSE}
library(p8105.datasets)
library(tidyverse)

# Set how graphs are printed in knitted file
knitr::opts_chunk$set(
  fig.width = 10,
  fig.asp = .6,
  out.width = "90%"
)

#Set global theme for plots
theme_set(theme_minimal() + theme(legend.position = "bottom"))

#Set options for all plots
options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

#Set options for all plots
scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```

## Problem 1 

Describe the data: 
```{r}
data("instacart")
```
**comments**  
The dataset instacart contains `r nrow(instacart)` observations and `r ncol(instacart)` columns. Observations are at the level of items in orders placed by users. The variables provide information about the users/orders and the items purchased. For example aisle contains values such as "fresh vegetables" and "eggs," product_name contains values such as "spring water" and "organic half and half" and days_since_prior_order is numeric. The total list of variables is: `r colnames(instacart)`. 

Count aisles: 
```{r}
instacart %>% 
  count(aisle) %>% 
  arrange(desc(n))
```
**comments**  
There are 134 aisles, with most items ordered from the fresh fruits and fresh vegetables aisles. 

Make a plot of aisles: 
```{r}
instacart %>% 
  count(aisle) %>% 
  filter(n > 10000) %>% 
  mutate(
    aisle = factor(aisle), 
    aisle = fct_reorder(aisle, n)
    ) %>% 
  ggplot(aes(x = aisle, y = n)) + 
  geom_point() + 
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1))
```
**comments**  
Most of the aisles with order counts above 10,000 show between 10k - 20k orders. There are about 8 aisles with order counts between 20-40k, 3 aisles with order counts between 40-80k, and two aisles (fruit and vegetables) with order counts around 150k.

Make a table of popular items:
```{r}
instacart %>% 
  filter(aisle %in% c("baking ingredients", "dog food care", "packaged vegetables fruits")) %>% 
  group_by(aisle) %>% 
  count(product_name) %>% 
  mutate(rank = min_rank(desc(n))) %>% 
  filter(rank < 4) %>% 
  arrange(aisle, rank) %>% 
  knitr::kable()
```
**comments**  
The most popular items in baking are brown sugar, baking soda, and cane sugar. The most popular items in dog food care are snack sticks, organix chicken recipe, and small dog biscuits. The most popular items in packaged fruits/veggies are baby spinach, raspberries, and blueberries. 

Make a table for mean hour of product ordered on each day of the week:
```{r}
instacart %>% 
  filter(product_name %in% c("Pink Lady Apples", "Coffee Ice Cream")) %>% 
  group_by(product_name, order_dow) %>% 
  summarize(mean_hour = mean(order_hour_of_day)) %>% 
  pivot_wider(
    names_from = order_dow, 
    values_from = mean_hour
  )
```
**comments**  
Overall coffee ice cream is ordered a little later in the day on weekdays (around 3pm), while pink lady apples are ordered a little earlier in the day (around noon). On weekends both products on average are ordered around 12pm. 

## Problem 2

Load data and tidy by cleaning variable names and pivoting to longer with one variable for the minute of activity in the day and one variable for the activity count in that minute. Add a weekend and weekday variable and change minute of activity to numeric. 
```{r, message = FALSE}
accel_df <- read_csv("./data/accel_data.csv") %>% 
  janitor::clean_names() %>% 
  pivot_longer(
    activity_1:activity_1440,
    names_to = "minute_activity", 
    values_to = "activity_count") %>% 
    {mutate(., weekend = (ifelse(pull(., day_id) %in% c("3", "4"), 
                                 "weekend", "weekday")))} %>% 
  mutate(minute_activity = rep((1:1440), times = 35))
```
**comments**  
The dataset accel_df contains `r nrow(accel_df)` observations and `r ncol(accel_df)` variables. Variables are the week, the day, the specific minute of the day, the activity count in that minute, and whether it was a weekend or a weekday. 

Create a table with daily activity counts summed across all minutes for the 5 weeks. 
**FACTOR RELVEL NOT WORKING**
```{r, message = FALSE}
accel_df %>% 
  group_by(week, day) %>% 
  summarize(
    daily_activity = sum(activity_count)) %>% 
  mutate(day = as.factor(day)) %>% 
  mutate(day = (forcats::fct_relevel(day, c("Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday", "Sunday")))) %>% 
  pivot_wider(
    names_from = day, 
    values_from = daily_activity) %>% 
  knitr::kable()
```
**comments**  
From this summary it seems that in weeks 4 and 5 the activity on Sunday and particularly Saturday was lower than the other days and weeks. Other than that trends are not very apparent. 

Create a plot showing activity over the course of the day, with each day of the week represented by a different color.  
```{r, message = FALSE}
accel_df %>% 
  ggplot(aes(x = minute_activity, y = activity_count, color = day)) + 
  geom_point(size = .1, alpha = .5) +
  geom_line(size = .2) + 
  geom_smooth(size = .2)
```

**comments**  
Based on this graph I can see that the first ~300 minutes of the day have much less activity than the rest of the day. There are some spikes in activity around 500-700 minutes (especially for Sundayand) and around 1300-1400 minutes. The spikes later in the day are particularly pronounced on Fridays.

## Problem 3

Describe the dataset: 
```{r}
data("ny_noaa")
miss_prcp <-
  ny_noaa %>% 
  drop_na(prcp)
miss_snow <-
  ny_noaa %>% 
  drop_na(snow)
miss_snwd <-
  ny_noaa %>% 
  drop_na(snwd)
miss_tmax <-
  ny_noaa %>% 
  drop_na(tmax)
miss_tmin <-
  ny_noaa %>% 
  drop_na(tmin)
```
The dataset ny_noaa contains `r nrow(ny_noaa)` observations and `r ncol(ny_noaa)` variables. Variables include: `r colnames(ny_noaa)`. It includes data from `r min(pull(ny_noaa, date))` to `r max(pull(ny_noaa, date))`. There are `r nrow(ny_noaa) - nrow(miss_prcp)` missing prcp values, `r nrow(ny_noaa) - nrow(miss_snow)` missing snow values, `r nrow(ny_noaa) - nrow(miss_snwd)` missing snwd values, `r nrow(ny_noaa) - nrow(miss_tmax)` missing tmax values, and `r nrow(ny_noaa) - nrow(miss_tmin)` missing tmin values. Given this, it appears that about 5% of the data are missing for each variable. 

Do some data cleaning: create separate variables for year/month/day, divide temperature, snowfall, and snow depth by 10 to result in cm, divide precipitation by 100 to result in cm. 
```{r}
tidy_ny_noaa <- 
  ny_noaa %>% 
  separate(date, c("Year", "Month", "Day"), convert = TRUE) %>% 
  mutate(prcp = prcp/100) %>% 
  mutate(snow = snow/10) %>% 
  mutate(snwd = snwd/10) %>% 
  mutate(tmax = as.numeric(tmax)/10) %>% 
  mutate(tmin = as.numeric(tmin)/10) 

tidy_ny_noaa %>% 
  count(snow, sort = TRUE)
```
The most commonly observed values for snowfall are 0 and NA. This is likely because overall there were a lot of missing values, and because NY only gets snow a few weeks out of the year. 

Make a two-panel plot showing the average max temperature in January and in July in each station across years. Is there any observable / interpretable structure? Any outliers? 
```{r}
tidy_ny_noaa %>% 
  group_by(id, Year, Month) %>% 
  filter(Month %in% c(1, 7)) %>% 
  recode(Month)
  summarize(
    avg_max_temp = mean(tmax, na.rm = TRUE)) %>% 
  ggplot(aes(x = Year, y = avg_max_temp, group = id)) +
  geom_point(size = .1) +
  geom_path(size = .1) +
  #geom_smooth() +
  facet_grid(. ~ Month) 
  
```


Make a two-panel plot showing (i) tmax vs tmin for the full dataset (note that a scatterplot may not be the best option); and (ii) make a plot showing the distribution of snowfall values greater than 0 and less than 100 separately by year.
use patchwork to merge plots. Don't use scatterplot, maybe use contour plot, bin plot, hex plot... 
First filter, then show distribution (ridge, box, violin, etc...) for each year. 
```{r}

```

